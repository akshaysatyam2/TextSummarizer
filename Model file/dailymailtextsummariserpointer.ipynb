{"metadata":{"kernelspec":{"name":"python3","display_name":"Python 3","language":"python"},"language_info":{"name":"python","version":"3.10.12","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"}},"nbformat_minor":5,"nbformat":4,"cells":[{"cell_type":"markdown","source":"# Text Summarizer","metadata":{}},{"cell_type":"code","source":"import numpy as np\nimport pandas as pd\nimport tensorflow as tf","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Data Preprocessing","metadata":{}},{"cell_type":"code","source":"dataset = pd.read_csv('/kaggle/input/dailymailstoriescsv/DailymailStories.csv', nrows= 50000, usecols=[1,2,3])","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"dataset.head()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"dataset.info()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"dataset.describe()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"dataset.iloc[2,1]","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"pip install gensim","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"import gensim\ndataset['cleanText'] = dataset['text'].apply(lambda x: gensim.utils.simple_preprocess(x))","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"dataset.head()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"dataset['cleanSummary'] = dataset['summary'].apply(lambda x: gensim.utils.simple_preprocess(x))","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"dataset.head()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"dataset.info()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"dataset.describe()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"dataset.iloc[0, 4]","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"from sklearn.model_selection import train_test_split\nX_train, X_test, y_train, y_test = train_test_split(np.array(dataset['cleanText']), np.array(dataset['cleanSummary']), test_size = 0.2, random_state = 123)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"X_train","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"X_test","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"y_train","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"y_test","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"max_text_len = 500\nmax_summary_len = 100","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"pip install nltk","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"import nltk\nnltk.download('stopwords')","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"import re\nfrom bs4 import BeautifulSoup\nfrom keras.preprocessing.text import Tokenizer\nfrom tensorflow.keras.preprocessing.sequence import pad_sequences\nfrom nltk.corpus import stopwords\nfrom tensorflow.keras.layers import LSTM, Embedding, Dense\nfrom tensorflow.keras.models import Model\nfrom tensorflow.keras.callbacks import EarlyStopping\nimport warnings\n\npd.set_option( \"display.max_columns\", 500)\nwarnings.filterwarnings(\"ignore\")","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"x_tokenizer = Tokenizer()\nX_tokenizer = Tokenizer() \nX_train = X_train.flatten()\nX_tokenizer.fit_on_texts(X_train)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"thresh = 4\n\ncnt = 0\n\ntot_cnt = 0\n\nfreq = 0\ntot_freq = 0\n\nfor key,value in X_tokenizer.word_counts.items():\n    tot_cnt=tot_cnt+1\n    tot_freq=tot_freq+value\n    if(value<thresh):\n        cnt=cnt+1\n        freq=freq+value\n    \nprint(\"% of rare words in vocabulary:\",(cnt/tot_cnt)*100)\nprint(\"Total Coverage of rare words:\",(freq/tot_freq)*100)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"X_tokenizer = Tokenizer(num_words=tot_cnt-cnt) \n\nX_tokenizer.fit_on_texts(list(X_train))\n\nX_train_seq = X_tokenizer.texts_to_sequences(X_train) \nX_test_seq = X_tokenizer.texts_to_sequences(X_test)\n\nX_train = pad_sequences(X_train_seq,  maxlen = max_text_len, padding = 'post')\nX_test = pad_sequences(X_test_seq, maxlen = max_text_len, padding = 'post')\n\nX_voc = X_tokenizer.num_words + 1","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"X_voc ","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"y_tokenizer = Tokenizer()   \ny_tokenizer.fit_on_texts(list(y_train))","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"\nthresh = 6\n\ncnt = 0\ntot_cnt = 0\nfreq = 0\ntot_freq = 0\n\nfor key,value in y_tokenizer.word_counts.items():\n    tot_cnt = tot_cnt+1\n    tot_freq = tot_freq+value\n    if(value<thresh):\n        cnt = cnt+1\n        freq = freq+value\n    \nprint(\"% of rare words in vocabulary:\",(cnt/tot_cnt)*100)\nprint(\"Total Coverage of rare words:\",(freq/tot_freq)*100)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"y_tokenizer = Tokenizer(num_words=tot_cnt-cnt)\n\ny_tokenizer.fit_on_texts(list(y_train))\n\ny_train_seq = y_tokenizer.texts_to_sequences(y_train) \ny_test_seq = y_tokenizer.texts_to_sequences(y_test) \n\ny_train = pad_sequences(y_train_seq, maxlen=max_summary_len, padding='post')\ny_test = pad_sequences(y_test_seq, maxlen=max_summary_len, padding='post')\n\ny_voc = y_tokenizer.num_words +1","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"y_voc","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"ind=[]\nfor i in range(len(y_train)):\n    cnt=0\n    for j in y_train[i]:\n        if j!=0:\n            cnt=cnt+1\n    if(cnt==2):\n        ind.append(i)\n\ny_train=np.delete(y_train,ind, axis=0)\nX_train=np.delete(X_train,ind, axis=0)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"ind=[]\nfor i in range(len(y_test)):\n    cnt=0\n    for j in y_test[i]:\n        if j!=0:\n            cnt=cnt+1\n    if(cnt==2):\n        ind.append(i)\n\ny_test=np.delete(y_test,ind, axis=0)\nX_test=np.delete(X_test,ind, axis=0)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"import os\nfrom tensorflow.keras.layers import Layer\nfrom tensorflow.keras import backend as K\n\n\nclass AttentionLayer(Layer):\n    def __init__(self, **kwargs):\n        super(AttentionLayer, self).__init__(**kwargs)\n\n    def build(self, input_shape):\n        assert isinstance(input_shape, list)\n\n        self.W_a = self.add_weight(name='W_a',\n                                   shape=tf.TensorShape((input_shape[0][2], input_shape[0][2])),\n                                   initializer='uniform',\n                                   trainable=True)\n        self.U_a = self.add_weight(name='U_a',\n                                   shape=tf.TensorShape((input_shape[1][2], input_shape[0][2])),\n                                   initializer='uniform',\n                                   trainable=True)\n        self.V_a = self.add_weight(name='V_a',\n                                   shape=tf.TensorShape((input_shape[0][2], 1)),\n                                   initializer='uniform',\n                                   trainable=True)\n\n        super(AttentionLayer, self).build(input_shape)\n\n    def call(self, inputs, verbose=False):\n        assert type(inputs) == list\n        encoder_out_seq, decoder_out_seq = inputs\n        if verbose:\n            print('encoder_out_seq>', encoder_out_seq.shape)\n            print('decoder_out_seq>', decoder_out_seq.shape)\n\n        def energy_step(inputs, states):\n\n            assert_msg = \"States must be a list. However states {} is of type {}\".format(states, type(states))\n            assert isinstance(states, list) or isinstance(states, tuple), assert_msg\n\n            en_seq_len, en_hidden = encoder_out_seq.shape[1], encoder_out_seq.shape[2]\n            de_hidden = inputs.shape[-1]\n\n            # <= batch_size*en_seq_len, latent_dim\n            reshaped_enc_outputs = K.reshape(encoder_out_seq, (-1, en_hidden))\n            \n            # <= batch_size*en_seq_len, latent_dim\n            W_a_dot_s = K.reshape(K.dot(reshaped_enc_outputs, self.W_a), (-1, en_seq_len, en_hidden))\n            \n            if verbose:\n                print('wa.s>',W_a_dot_s.shape)\n\n            U_a_dot_h = K.expand_dims(K.dot(inputs, self.U_a), 1)  # <= batch_size, 1, latent_dim\n            if verbose:\n                print('Ua.h>',U_a_dot_h.shape)\n\n            # <= batch_size*en_seq_len, latent_dim\n            reshaped_Ws_plus_Uh = K.tanh(K.reshape(W_a_dot_s + U_a_dot_h, (-1, en_hidden)))\n            \n            if verbose:\n                print('Ws+Uh>', reshaped_Ws_plus_Uh.shape)\n\n            # <= batch_size, en_seq_len\n            e_i = K.reshape(K.dot(reshaped_Ws_plus_Uh, self.V_a), (-1, en_seq_len))\n            # <= batch_size, en_seq_len\n            e_i = K.softmax(e_i)\n\n            if verbose:\n                print('ei>', e_i.shape)\n\n            return e_i, [e_i]\n\n        def context_step(inputs, states):\n            \n            # <= batch_size, hidden_size\n            c_i = K.sum(encoder_out_seq * K.expand_dims(inputs, -1), axis=1)\n            if verbose:\n                print('ci>', c_i.shape)\n            return c_i, [c_i]\n\n        def create_inital_state(inputs, hidden_size):\n            fake_state = K.zeros_like(inputs)\n            fake_state = K.sum(fake_state, axis=[1, 2])\n            fake_state = K.expand_dims(fake_state)\n            fake_state = K.tile(fake_state, [1, hidden_size])\n            return fake_state\n\n        fake_state_c = create_inital_state(encoder_out_seq, encoder_out_seq.shape[-1])\n        fake_state_e = create_inital_state(encoder_out_seq, encoder_out_seq.shape[1])\n        \n        # e_outputs => (batch_size, de_seq_len, en_seq_len)\n        last_out, e_outputs, _ = K.rnn(\n            energy_step, decoder_out_seq, [fake_state_e],\n        )\n\n        last_out, c_outputs, _ = K.rnn(\n            context_step, e_outputs, [fake_state_c],\n        )\n\n        return c_outputs, e_outputs\n\n    def compute_output_shape(self, input_shape):\n        return [\n            tf.TensorShape((input_shape[1][0], input_shape[1][1], input_shape[1][2])),\n            tf.TensorShape((input_shape[1][0], input_shape[1][1], input_shape[0][1]))\n        ]","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"import tensorflow as tf\nfrom tensorflow.keras.layers import Input, Embedding, LSTM, Dense, Attention, Concatenate, AdditiveAttention\nfrom tensorflow.keras.models import Model\nfrom tensorflow.keras.layers import Add\n\ntf.keras.backend.clear_session()\n\nlatent_dim = 256\nembedding_dim = 256\n\n# Encoder\nencoder_inputs = Input(shape=(max_text_len,))\nenc_emb = Embedding(X_voc, embedding_dim, trainable=True)(encoder_inputs)\nencoder_lstm1 = LSTM(latent_dim, return_sequences=True, return_state=True, dropout=0.2, recurrent_dropout=0.2)\nencoder_output1, state_h1, state_c1 = encoder_lstm1(enc_emb)\nencoder_lstm2 = LSTM(latent_dim, return_sequences=True, return_state=True, dropout=0.2, recurrent_dropout=0.2)\nencoder_output2, state_h2, state_c2 = encoder_lstm2(encoder_output1)\nencoder_lstm3 = LSTM(latent_dim, return_state=True, return_sequences=True, dropout=0.2, recurrent_dropout=0.2)\nencoder_outputs, state_h, state_c = encoder_lstm3(encoder_output2)\n\n# Decoder\ndecoder_inputs = Input(shape=(None,))\ndec_emb_layer = Embedding(y_voc, embedding_dim, trainable=True)\ndec_emb = dec_emb_layer(decoder_inputs)\ndecoder_lstm = LSTM(latent_dim, return_sequences=True, return_state=True, dropout=0.4, recurrent_dropout=0.2)\ndecoder_outputs, decoder_fwd_state, decoder_back_state = decoder_lstm(dec_emb, initial_state=[state_h, state_c])\n\n# Attention Mechanism\nattn_layer = AttentionLayer(name='attention_layer')\nattn_out, attn_states = attn_layer([encoder_outputs, decoder_outputs])\n\n# Pointer Generator\npointer_layer = Dense(1, activation='sigmoid', name='pointer_generator')\npointer_input = Concatenate(axis=-1)([attn_out, decoder_outputs])\npointer_scores = pointer_layer(pointer_input)\n\n# Calculate context vector\ncontext_vector = tf.keras.layers.Dot(axes=[1, 1])([attn_out, pointer_scores])\ncontext_decoder_input = Concatenate(axis=-1)([context_vector, decoder_outputs])\n\n# Final output layer\ndecoder_concat_input = Concatenate(axis=-1, name='concat_layer')([context_decoder_input, attn_out])\ndecoder_dense = tf.keras.layers.TimeDistributed(Dense(y_voc, activation='softmax'))\ndecoder_outputs = decoder_dense(decoder_concat_input)\n\nmodel = Model([encoder_inputs, decoder_inputs], decoder_outputs)\n","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"model.compile(optimizer='rmsprop', loss='sparse_categorical_crossentropy' , metrics=['accuracy'])","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"es = EarlyStopping(monitor='val_loss', mode='min', verbose=1)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"history = model.fit([X_train,y_train[:,:-1]], y_train.reshape(y_train.shape[0],y_train.shape[1], 1)[:,1:] ,epochs=10,callbacks=[es],batch_size= 64, validation_data=([X_test,y_test[:,:-1]], y_test.reshape(y_test.shape[0],y_test.shape[1], 1)[:,1:]))","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"from matplotlib import pyplot\npyplot.plot(history.history['accuracy'], label='train') \npyplot.plot(history.history['val_accuracy'], label='test') \npyplot.legend() \npyplot.show()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"pyplot.plot(history.history['loss'], label='train') \npyplot.plot(history.history['val_loss'], label='test') \npyplot.legend() \npyplot.show()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"reverse_target_word_index=y_tokenizer.index_word \nreverse_source_word_index=X_tokenizer.index_word \nfrom tensorflow.keras.layers import AdditiveAttention, Attention\ntarget_word_index=y_tokenizer.word_index","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"from tensorflow.keras.layers import Input, Concatenate\n\nencoder_model = Model(inputs=encoder_inputs,outputs=[encoder_outputs, state_h, state_c])\n\ndecoder_state_input_h = Input(shape=(latent_dim,))\ndecoder_state_input_c = Input(shape=(latent_dim,))\ndecoder_hidden_state_input = Input(shape=(max_text_len,latent_dim))\n\ndec_emb2= dec_emb_layer(decoder_inputs) \n\ndecoder_outputs2, state_h2, state_c2 = decoder_lstm(dec_emb2, initial_state=[decoder_state_input_h, decoder_state_input_c])\n\nattn_out_inf, attn_states_inf = attn_layer([decoder_hidden_state_input, decoder_outputs2])\n\ndecoder_inf_concat = Concatenate(axis=-1, name='concat')([decoder_outputs2, attn_out_inf])\n\ndecoder_outputs2 = decoder_dense(decoder_inf_concat) \n\ndecoder_model = Model(\n    [decoder_inputs] + [decoder_hidden_state_input,decoder_state_input_h, decoder_state_input_c],\n    [decoder_outputs2] + [state_h2, state_c2])","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"def decode_sequence(input_seq):\n    e_out, e_h, e_c = encoder_model.predict(input_seq)\n    \n    target_seq = np.zeros((1,1))\n    \n    target_seq[0, 0] = target_word_index['sostok']\n\n    stop_condition = False\n    decoded_sentence = ''\n    while not stop_condition:\n      \n        output_tokens, h, c = decoder_model.predict([target_seq] + [e_out, e_h, e_c])\n\n        sampled_token_index = np.argmax(output_tokens[0, -1, :])\n        sampled_token = reverse_target_word_index[sampled_token_index]\n        \n        if(sampled_token!='eostok'):\n            decoded_sentence += ' '+sampled_token\n\n        if (sampled_token == 'eostok'  or len(decoded_sentence.split()) >= (max_summary_len-1)):\n            stop_condition = True\n\n        target_seq = np.zeros((1,1))\n        target_seq[0, 0] = sampled_token_index\n\n        e_h, e_c = h, c\n\n    return decoded_sentence","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"def seq2summary(input_seq):\n    newString=''\n    for i in input_seq:\n        if((i!=0 and i!=target_word_index['sostok']) and i!=target_word_index['eostok']):\n            newString=newString+reverse_target_word_index[i]+' '\n    return newString\n\ndef seq2text(input_seq):\n    newString=''\n    for i in input_seq:\n        if(i!=0):\n            newString=newString+reverse_source_word_index[i]+' '\n    return newString","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"for i in range(0,4):\n    print(\"Review:\",seq2text(X_train[i]))\n    print(\"Original summary:\",seq2summary(y_train[i]))\n    print(\"Predicted summary:\",decode_sequence(X_train[i].reshape(1,max_text_len)))\n    print(\"\\n\")","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"from nltk.translate.bleu_score import sentence_bleu\nfor i in range(0,1000):\n  reference = seq2summary(y_train[i])\n  candidate = decode_sequence(X_train[i].reshape(1, max_text_len))\n\nprint('Individual 1-gram: %f' % sentence_bleu(reference, candidate, weights=(1, 0, 0, 0)))\nprint('Individual 2-gram: %f' % sentence_bleu(reference, candidate, weights=(0, 1, 0, 0)))\nprint('Individual 3-gram: %f' % sentence_bleu(reference, candidate, weights=(0, 0, 1, 0)))\nprint('Individual 4-gram: %f' % sentence_bleu(reference, candidate, weights=(0, 0, 0, 1)))","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"from nltk.translate.bleu_score import sentence_bleu\nfor i in range(0,1000):\n  reference = seq2summary(y_train[i])\n  candidate = decode_sequence(X_train[i].reshape(1, max_text_len))\n\nscore = sentence_bleu(reference, candidate, weights=(0.25, 0.25, 0.25, 0.25))\nprint(score)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"from nltk.translate.bleu_score import sentence_bleu\nfor i in range(0,1000):\n  reference = seq2summary(y_train[i])\n  candidate = decode_sequence(X_train[i].reshape(1, max_text_len))\n\nprint('Cumulative 1-gram: %f' % sentence_bleu(reference, candidate, weights=(1, 0, 0, 0)))\nprint('Cumulative 2-gram: %f' % sentence_bleu(reference, candidate, weights=(0.5, 0.5, 0, 0)))\nprint('Cumulative 3-gram: %f' % sentence_bleu(reference, candidate, weights=(0.33, 0.33, 0.33, 0)))\nprint('Cumulative 4-gram: %f' % sentence_bleu(reference, candidate, weights=(0.25, 0.25, 0.25, 0.25)))","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"from nltk.translate.bleu_score import sentence_bleu\nfor i in range(0,1000):\n  reference = seq2summary(y_test[i])\n  candidate = decode_sequence(X_test[i].reshape(1, max_text_len))\nprint(\"Test/Validation Set :\")\nprint('Individual 1-gram: %f' % sentence_bleu(reference, candidate, weights=(1, 0, 0, 0)))\nprint('Individual 2-gram: %f' % sentence_bleu(reference, candidate, weights=(0, 1, 0, 0)))\nprint('Individual 3-gram: %f' % sentence_bleu(reference, candidate, weights=(0, 0, 1, 0)))\nprint('Individual 4-gram: %f' % sentence_bleu(reference, candidate, weights=(0, 0, 0, 1)))","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"from nltk.translate.bleu_score import sentence_bleu\nfor i in range(0,1000):\n  reference = seq2summary(y_test[i])\n  candidate = decode_sequence(X_test[i].reshape(1, max_text_len))\n\nscore = sentence_bleu(reference, candidate, weights=(0.25, 0.25, 0.25, 0.25))\nprint(score)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"from nltk.translate.bleu_score import sentence_bleu\nfor i in range(0,1000):\n  reference = seq2summary(y_test[i])\n  candidate = decode_sequence(X_test[i].reshape(1, max_text_len))\n\nprint('Cumulative 1-gram: %f' % sentence_bleu(reference, candidate, weights=(1, 0, 0, 0)))\nprint('Cumulative 2-gram: %f' % sentence_bleu(reference, candidate, weights=(0.5, 0.5, 0, 0)))\nprint('Cumulative 3-gram: %f' % sentence_bleu(reference, candidate, weights=(0.33, 0.33, 0.33, 0)))\nprint('Cumulative 4-gram: %f' % sentence_bleu(reference, candidate, weights=(0.25, 0.25, 0.25, 0.25)))","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"!pip install evaluate\n!pip install rouge-score","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"import evaluate\nrouge = evaluate.load('rouge')\nfor i in range(0,4):\n    print(\"Review:\",seq2text(X_train[i]))\n    print(\"Original summary:\",seq2summary(y_train[i]))\n    print(\"Predicted summary:\",decode_sequence(X_train[i].reshape(1,max_text_len)))\n    results = rouge.compute(predictions=decode_sequence(X_train[i].reshape(1,max_text_len)), references=seq2summary(y_train[i]))\n    print(results)\n    print(\"\\n\")","metadata":{"trusted":true},"execution_count":null,"outputs":[]}]}